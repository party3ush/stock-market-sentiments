{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-05-26T16:00:45.092078Z","iopub.execute_input":"2023-05-26T16:00:45.092471Z","iopub.status.idle":"2023-05-26T16:00:45.139191Z","shell.execute_reply.started":"2023-05-26T16:00:45.092439Z","shell.execute_reply":"2023-05-26T16:00:45.137963Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\nfrom collections import Counter\nimport re\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom wordcloud import WordCloud\nimport spacy\nimport nltk\nfrom nltk.stem import PorterStemmer\nfrom nltk.corpus import stopwords","metadata":{"execution":{"iopub.status.busy":"2023-05-26T16:00:45.599239Z","iopub.execute_input":"2023-05-26T16:00:45.600029Z","iopub.status.idle":"2023-05-26T16:01:01.797780Z","shell.execute_reply.started":"2023-05-26T16:00:45.599982Z","shell.execute_reply":"2023-05-26T16:01:01.796900Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# DATA PreProcessing","metadata":{}},{"cell_type":"code","source":"df=pd.read_csv('/kaggle/input/stockmarket-sentiment-dataset/stock_data.csv')\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2023-05-26T16:01:01.799333Z","iopub.execute_input":"2023-05-26T16:01:01.800906Z","iopub.status.idle":"2023-05-26T16:01:01.864893Z","shell.execute_reply.started":"2023-05-26T16:01:01.800856Z","shell.execute_reply":"2023-05-26T16:01:01.863633Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['Text'] = df['Text'].apply(lambda x: re.sub(r'\\W', ' ', str(x))) # remove non-alphanumeric characters","metadata":{"execution":{"iopub.status.busy":"2023-05-26T16:01:01.866161Z","iopub.execute_input":"2023-05-26T16:01:01.866484Z","iopub.status.idle":"2023-05-26T16:01:01.933165Z","shell.execute_reply.started":"2023-05-26T16:01:01.866456Z","shell.execute_reply":"2023-05-26T16:01:01.931994Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['Text'] = df['Text'].apply(lambda x: re.sub(r'\\s+[a-zA-Z]\\s+', ' ', x)) # remove single character words","metadata":{"execution":{"iopub.status.busy":"2023-05-26T16:01:01.935890Z","iopub.execute_input":"2023-05-26T16:01:01.936265Z","iopub.status.idle":"2023-05-26T16:01:01.987327Z","shell.execute_reply.started":"2023-05-26T16:01:01.936232Z","shell.execute_reply":"2023-05-26T16:01:01.986263Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['Text'] = df['Text'].apply(lambda x: re.sub(r'\\^[a-zA-Z]\\s+', ' ', x)) # remove single characters at the beginning of text","metadata":{"execution":{"iopub.status.busy":"2023-05-26T16:01:01.988860Z","iopub.execute_input":"2023-05-26T16:01:01.989276Z","iopub.status.idle":"2023-05-26T16:01:02.006738Z","shell.execute_reply.started":"2023-05-26T16:01:01.989170Z","shell.execute_reply":"2023-05-26T16:01:02.005561Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['Text'] = df['Text'].apply(lambda x: re.sub(r'\\s+', ' ', x, flags=re.I)) # replace multiple spaces with single space","metadata":{"execution":{"iopub.status.busy":"2023-05-26T16:01:02.008367Z","iopub.execute_input":"2023-05-26T16:01:02.008848Z","iopub.status.idle":"2023-05-26T16:01:02.078144Z","shell.execute_reply.started":"2023-05-26T16:01:02.008791Z","shell.execute_reply":"2023-05-26T16:01:02.076830Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['Text'] = df['Text'].apply(lambda x: x.lower()) # convert to lowercase","metadata":{"execution":{"iopub.status.busy":"2023-05-26T16:01:02.080066Z","iopub.execute_input":"2023-05-26T16:01:02.080522Z","iopub.status.idle":"2023-05-26T16:01:02.090694Z","shell.execute_reply.started":"2023-05-26T16:01:02.080476Z","shell.execute_reply":"2023-05-26T16:01:02.089333Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#remove stop words and puncuations\n\nnlp = spacy.load('en_core_web_sm')\nstopwords = nlp.Defaults.stop_words\ndf['text_processed'] = df['Text'].apply(lambda x: ' '.join([token.text for token in nlp(x) if not token.is_stop and not token.is_punct]))\n","metadata":{"execution":{"iopub.status.busy":"2023-05-26T16:01:02.091910Z","iopub.execute_input":"2023-05-26T16:01:02.092227Z","iopub.status.idle":"2023-05-26T16:01:56.037957Z","shell.execute_reply.started":"2023-05-26T16:01:02.092190Z","shell.execute_reply":"2023-05-26T16:01:56.036768Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2023-05-26T16:01:56.039412Z","iopub.execute_input":"2023-05-26T16:01:56.039786Z","iopub.status.idle":"2023-05-26T16:01:56.051508Z","shell.execute_reply.started":"2023-05-26T16:01:56.039754Z","shell.execute_reply":"2023-05-26T16:01:56.050289Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from nltk.corpus import stopwords\n\nnltk.download('stopwords')\nps = PorterStemmer()\ndf['Text'] = df['Text'].apply(lambda x: ' '.join([ps.stem(word) for word in x.split() if word not in set(stopwords.words('english'))]))\ndf['text_processed'] = df['text_processed'].apply(lambda x: ' '.join([ps.stem(word) for word in x.split() if word not in set(stopwords.words('english'))]))","metadata":{"execution":{"iopub.status.busy":"2023-05-26T16:01:56.056218Z","iopub.execute_input":"2023-05-26T16:01:56.056714Z","iopub.status.idle":"2023-05-26T16:02:20.854119Z","shell.execute_reply.started":"2023-05-26T16:01:56.056678Z","shell.execute_reply":"2023-05-26T16:02:20.853028Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#randomization\ndf = df.sample(frac=1).reset_index(drop=True)\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2023-05-26T16:02:20.855352Z","iopub.execute_input":"2023-05-26T16:02:20.855731Z","iopub.status.idle":"2023-05-26T16:02:20.874661Z","shell.execute_reply.started":"2023-05-26T16:02:20.855700Z","shell.execute_reply":"2023-05-26T16:02:20.873436Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['Sentiment']","metadata":{"execution":{"iopub.status.busy":"2023-05-26T16:02:20.875956Z","iopub.execute_input":"2023-05-26T16:02:20.876345Z","iopub.status.idle":"2023-05-26T16:02:20.886458Z","shell.execute_reply.started":"2023-05-26T16:02:20.876302Z","shell.execute_reply":"2023-05-26T16:02:20.885182Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Tfid vectorization","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import LabelEncoder\n\nX = df['Text']\ny = df['Sentiment']\n\nvectorizer = TfidfVectorizer()\nX = vectorizer.fit_transform(X)\n\nlabel_encoder = LabelEncoder()\ny.loc[y == -1] = 0 \ny= label_encoder.fit_transform(y)\n","metadata":{"execution":{"iopub.status.busy":"2023-05-26T16:02:20.888299Z","iopub.execute_input":"2023-05-26T16:02:20.888864Z","iopub.status.idle":"2023-05-26T16:02:21.046409Z","shell.execute_reply.started":"2023-05-26T16:02:20.888777Z","shell.execute_reply":"2023-05-26T16:02:21.045393Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X","metadata":{"execution":{"iopub.status.busy":"2023-05-26T16:02:21.047835Z","iopub.execute_input":"2023-05-26T16:02:21.048235Z","iopub.status.idle":"2023-05-26T16:02:21.055550Z","shell.execute_reply.started":"2023-05-26T16:02:21.048204Z","shell.execute_reply":"2023-05-26T16:02:21.054322Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = X.toarray()","metadata":{"execution":{"iopub.status.busy":"2023-05-26T16:02:21.057335Z","iopub.execute_input":"2023-05-26T16:02:21.058195Z","iopub.status.idle":"2023-05-26T16:02:21.330258Z","shell.execute_reply.started":"2023-05-26T16:02:21.058152Z","shell.execute_reply":"2023-05-26T16:02:21.329281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.callbacks import EarlyStopping\n\n# Creates 'EarlyStopping' callback\nearlystopping_cb = EarlyStopping(patience=4, restore_best_weights=True)","metadata":{"execution":{"iopub.status.busy":"2023-05-26T16:02:21.331757Z","iopub.execute_input":"2023-05-26T16:02:21.332191Z","iopub.status.idle":"2023-05-26T16:02:21.339561Z","shell.execute_reply.started":"2023-05-26T16:02:21.332153Z","shell.execute_reply":"2023-05-26T16:02:21.338419Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y","metadata":{"execution":{"iopub.status.busy":"2023-05-26T16:02:21.341042Z","iopub.execute_input":"2023-05-26T16:02:21.341346Z","iopub.status.idle":"2023-05-26T16:02:21.355667Z","shell.execute_reply.started":"2023-05-26T16:02:21.341319Z","shell.execute_reply":"2023-05-26T16:02:21.354502Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# neural network","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\n\ninput_dim = X.shape[1]\n\n\nmodel = keras.Sequential()\nmodel.add(layers.Dense(64, input_dim=input_dim, activation='relu'))\nmodel.add(layers.Dense(32, activation='relu'))\nmodel.add(layers.Dense(1, activation='tanh'))\n\n\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n\nhistory=model.fit(X, y, epochs=20, batch_size=32, validation_split=0.2)\n\nplt.plot(history.history['loss'], label='Training Loss')\nplt.plot(history.history['val_loss'], label='Validation Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()\n\nplt.plot(history.history['accuracy'], label='Training Accuracy')\nplt.plot(history.history['val_accuracy'], label='Validation Accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-05-26T16:02:21.357222Z","iopub.execute_input":"2023-05-26T16:02:21.358316Z","iopub.status.idle":"2023-05-26T16:02:45.042110Z","shell.execute_reply.started":"2023-05-26T16:02:21.358273Z","shell.execute_reply":"2023-05-26T16:02:45.040986Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"execution":{"iopub.status.busy":"2023-05-26T16:02:45.043691Z","iopub.execute_input":"2023-05-26T16:02:45.044013Z","iopub.status.idle":"2023-05-26T16:02:45.062365Z","shell.execute_reply.started":"2023-05-26T16:02:45.043985Z","shell.execute_reply":"2023-05-26T16:02:45.061563Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_dim = X.shape[1]\n\nmodel = keras.Sequential()\nmodel.add(layers.Dense(128, input_dim=input_dim, activation='relu'))\nmodel.add(layers.Dense(64, activation='relu'))\nmodel.add(layers.Dense(32, activation='relu'))\nmodel.add(layers.Dense(1, activation='tanh'))\n\n\nmodel.compile(loss='hinge', optimizer='adam', metrics=['accuracy'])\n\n\nhistory=model.fit(X, y, epochs=20, batch_size=32, validation_split=0.2)\n\nplt.plot(history.history['loss'], label='Training Loss')\nplt.plot(history.history['val_loss'], label='Validation Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()\n\nplt.plot(history.history['accuracy'], label='Training Accuracy')\nplt.plot(history.history['val_accuracy'], label='Validation Accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-05-26T16:02:45.064056Z","iopub.execute_input":"2023-05-26T16:02:45.064391Z","iopub.status.idle":"2023-05-26T16:03:19.023797Z","shell.execute_reply.started":"2023-05-26T16:02:45.064360Z","shell.execute_reply":"2023-05-26T16:03:19.022497Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_dim = X.shape[1]\n\nmodel = keras.Sequential()\nmodel.add(layers.Dense(32, input_dim=input_dim, activation='relu'))\nmodel.add(layers.Dense(64, activation='relu'))\nmodel.add(layers.Dense(128, activation='relu'))\nmodel.add(layers.Dense(64, activation='relu'))\nmodel.add(layers.Dense(1, activation='tanh'))\n\nmodel.compile(loss='squared_hinge', optimizer='adam', metrics=['accuracy'])\n\nhistory=model.fit(X, y, epochs=20, batch_size=32, validation_split=0.2)\n\nplt.plot(history.history['loss'], label='Training Loss')\nplt.plot(history.history['val_loss'], label='Validation Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()\n\nplt.plot(history.history['accuracy'], label='Training Accuracy')\nplt.plot(history.history['val_accuracy'], label='Validation Accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-05-26T16:03:19.025709Z","iopub.execute_input":"2023-05-26T16:03:19.026104Z","iopub.status.idle":"2023-05-26T16:03:41.595477Z","shell.execute_reply.started":"2023-05-26T16:03:19.026072Z","shell.execute_reply":"2023-05-26T16:03:41.594591Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_dim = X.shape[1]\n\nmodel = keras.Sequential()\nmodel.add(layers.Dense(128, input_dim=input_dim, activation='relu'))\nmodel.add(layers.Dense(64, activation='relu'))\nmodel.add(layers.Dense(32, activation='relu'))\nmodel.add(layers.Dense(16, activation='relu'))\nmodel.add(layers.Dense(1, activation='tanh'))\n\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\nhistory = model.fit(X, y, epochs=20, batch_size=32, validation_split=0.2)\n\nplt.plot(history.history['loss'], label='Training Loss')\nplt.plot(history.history['val_loss'], label='Validation Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()\n\nplt.plot(history.history['accuracy'], label='Training Accuracy')\nplt.plot(history.history['val_accuracy'], label='Validation Accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-05-26T16:03:41.596962Z","iopub.execute_input":"2023-05-26T16:03:41.597560Z","iopub.status.idle":"2023-05-26T16:04:16.432683Z","shell.execute_reply.started":"2023-05-26T16:03:41.597525Z","shell.execute_reply":"2023-05-26T16:04:16.431311Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# BoW","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nimport tensorflow as tf\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n\n\nX = df['Text']\ny = df['Sentiment']\n\ntexts_train, texts_test, sentiments_train, sentiments_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nlabel_encoder = LabelEncoder()\nsentiments_train = label_encoder.fit_transform(sentiments_train)\nsentiments_test = label_encoder.transform(sentiments_test)\n\nvectorizer = CountVectorizer()\nfeatures_train = vectorizer.fit_transform(texts_train)\nfeatures_test = vectorizer.transform(texts_test)\n\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Dense(64, activation='relu', input_shape=(features_train.shape[1],)),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\n\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\nhistory = model.fit(features_train.toarray(), sentiments_train, epochs=10, batch_size=32, validation_split=0.2)\n\nplt.figure(figsize=(12, 4))\n\nplt.subplot(1, 2, 1)\nplt.plot(history.history['loss'], label='Training Loss')\nplt.plot(history.history['val_loss'], label='Validation Loss')\nplt.title('Model Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.subplot(1, 2, 2)\nplt.plot(history.history['accuracy'], label='Training Accuracy')\nplt.plot(history.history['val_accuracy'], label='Validation Accuracy')\nplt.title('Model Accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-05-26T16:04:16.434481Z","iopub.execute_input":"2023-05-26T16:04:16.435587Z","iopub.status.idle":"2023-05-26T16:04:28.193239Z","shell.execute_reply.started":"2023-05-26T16:04:16.435522Z","shell.execute_reply":"2023-05-26T16:04:28.191760Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"execution":{"iopub.status.busy":"2023-05-26T16:04:28.195327Z","iopub.execute_input":"2023-05-26T16:04:28.196386Z","iopub.status.idle":"2023-05-26T16:04:28.217612Z","shell.execute_reply.started":"2023-05-26T16:04:28.196340Z","shell.execute_reply":"2023-05-26T16:04:28.216734Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# LSTM","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM, Dense\nimport matplotlib.pyplot as plt\n\nX = df['Text']\ny = df['Sentiment']\n\n# Preprocessing\nvectorizer = TfidfVectorizer()\nX = vectorizer.fit_transform(X)\n\nlabel_encoder = LabelEncoder()\ny = label_encoder.fit_transform(y)\n\n# Splitting the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Reshape input data to be 3-dimensional for LSTM\nX_train = np.reshape(X_train.toarray(), (X_train.shape[0], 1, X_train.shape[1]))\nX_test = np.reshape(X_test.toarray(), (X_test.shape[0], 1, X_test.shape[1]))\n\n# Creating the LSTM model\nmodel = Sequential()\nmodel.add(LSTM(64, input_shape=(1, X_train.shape[2])))\nmodel.add(Dense(1, activation='sigmoid'))\n\n# Compiling the model\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n# Training the model\nhistory = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n\n# Plotting accuracy and loss curves\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('Model Accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()\n\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend(['Train', 'Test'], loc='upper right')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-05-26T16:08:45.130291Z","iopub.execute_input":"2023-05-26T16:08:45.130764Z","iopub.status.idle":"2023-05-26T16:09:29.882214Z","shell.execute_reply.started":"2023-05-26T16:08:45.130727Z","shell.execute_reply":"2023-05-26T16:09:29.881030Z"},"trusted":true},"execution_count":null,"outputs":[]}]}